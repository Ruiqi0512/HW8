{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490b4b51",
   "metadata": {},
   "source": [
    "1\n",
    "1.1 A classification decision tree is a type of predictive model used in machine learning and statistics for classifying data into categories. It works by breaking down a dataset into smaller subsets based on decision rules inferred from the data features, and it represents the decision-making process in the form of a tree structure.\n",
    "2.2 Types of Problems:\n",
    "Binary Classification:The target variable has two possible outcomes.\n",
    "Example: Spam email detection\n",
    "Multiclass Classification:The target variable has more than two possible outcomes.\n",
    "Example: Handwriting digit recognition (0-9).\n",
    "Imbalanced Classification:When one class is significantly more common than others, decision trees can still help.\n",
    "Example: Fraud detection in credit card transactions (Fraud/Legitimate).\n",
    "2.3 Root Node:The tree starts by evaluating all the data against the feature that best separates the target classes \n",
    "Internal Nodes:Each node applies a decision rule based on one feature (e.g., Cholesterol > 200).\n",
    "The data is split into subsets that follow the decision paths.\n",
    "Branches:Each branch represents a possible outcome of the decision rule. \n",
    "When a branch cannot be split further (or meets stopping criteria), it becomes a leaf node.\n",
    "Each leaf node contains a class label (e.g., \"Diabetes\") or a probability distribution over the classes.\n",
    "Final Classification:A new data point starts at the root node and traverses the tree by following the decision rules at each node.The path ends at a leaf node, which determines the predicted class for that data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633263df",
   "metadata": {},
   "source": [
    "chatbot link:https://chatgpt.com/share/673763ca-5158-8007-839b-6885ef72623c\n",
    "chatbot summery:A classification decision tree is a supervised learning model used to categorize data into classes. It splits data at each node using decision rules based on feature values, forming a tree structure with branches representing decisions and leaf nodes giving class predictions. Common applications include spam detection, fraud detection, and medical diagnosis (e.g., predicting diabetes based on age and cholesterol). Data is classified by traversing the tree from the root node, following decision rules, and ending at a leaf node. The tree is interpretable, handles non-linear relationships, and works for binary and multiclass problems, but it can overfit data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a3f85",
   "metadata": {},
   "source": [
    "2.1 Accuracy\n",
    "Definition: Measures the proportion of correct predictions, including true positives and true negatives, among the total population.  \n",
    "Example and Scenario: A weather forecast predicts sunny or rainy days. A weather app provides daily predictions for an entire month, and for users, both sunny and rainy days are equally important.  \n",
    "Reason: I think Accuracy is used because it gives an overall sense of how well the app performs, as long as the number of sunny and rainy days is balanced.  \n",
    "Rationale: Accuracy is most appropriate when false positives and false negatives carry similar importance, and the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5e68d",
   "metadata": {},
   "source": [
    "2.2 Sensitivity\n",
    "Definition: Measures the proportion of actual positives (true cases) that are correctly identified.  \n",
    "Example and Scenario: COVID-19 testing during a pandemic. A hospital screens patients for COVID-19. Missing a positive case (false negative) could lead to an outbreak and further transmission.  \n",
    "Reason: I think sensitivity is critical because it ensures all infected individuals identified  and is essential to prevent further transmission.  \n",
    "Rationale: Sensitivity is key in scenarios where detecting all actual positives is crucial, even at the expense of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f70ab",
   "metadata": {},
   "source": [
    "2.3 Specificity\n",
    "Definition: Measures the proportion of actual negatives (true negatives) that are correctly identified.  \n",
    "Example and Scenario: A luggage scanning system checks for explosives at an airport. Incorrectly flagging harmless luggage (false positives) causes delays and unnecessary inspections.  \n",
    "Reason: I think high specificity ensures that harmless luggage is correctly classified as safe, reducing excessive false alarms.  \n",
    "Rationale: Specificity is vital when false positives are more problematic than false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4068a",
   "metadata": {},
   "source": [
    "2.4 Precision\n",
    "Definition: Measures the proportion of predicted positives that are actually correct.  \n",
    "Example and Scenario: A recruitment tool identifies candidates most likely to succeed in a hiring process. If the tool incorrectly identifies unqualified candidates as top-tier (false positives), it wastes time and resources.  \n",
    "Reason: I think precision ensures that the candidates identified as top-tier are fit for the role, reducing wasted effort.  Rationale: Precision is prioritized when the cost of false positives is high, ensuring identified positives are highly reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea45279",
   "metadata": {},
   "source": [
    "chatbot link: https://chatgpt.com/c/673bdd8a-62bc-8007-a29f-920fa06ed896\n",
    "chatbot summery:Accuracy measures overall correctness, useful in balanced datasets. Sensitivity (recall) identifies true positives, critical in high-stakes scenarios like disease detection. Specificity ensures true negatives are correctly identified, ideal for avoiding false alarms in areas like security. Precision focuses on the reliability of positive predictions, important in scenarios like fraud detection. Accuracy is broad but can mislead with imbalanced data. Sensitivity minimizes false negatives, specificity minimizes false positives, and precision emphasizes correct positives. Each metric suits distinct decision-making needs: accuracy for general performance, sensitivity for “don’t miss it,” specificity for “don’t wrongly accuse,” and precision for “be sure it’s right.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "3\n",
    "# Step 1: Remove specified columns\n",
    "ab = ab.drop(columns=['Weight_oz', 'Width', 'Height'], errors='ignore')\n",
    "\n",
    "# Step 2: Drop rows with NaN entries\n",
    "ab_cleaned = ab.dropna()\n",
    "\n",
    "# Step 3: Redefine column types\n",
    "ab_cleaned['Pub year'] = ab_cleaned['Pub year'].astype(int)\n",
    "ab_cleaned['NumPages'] = ab_cleaned['NumPages'].astype(int)\n",
    "ab_cleaned['Hard_or_Paper'] = ab_cleaned['Hard_or_Paper'].astype('category')\n",
    "\n",
    "# Step 4: Verify the changes\n",
    "print(\"Cleaned Dataset Shape:\", ab_cleaned.shape)\n",
    "print(\"\\nData Types:\\n\", ab_cleaned.dtypes)\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "print(\"\\nPreview of Cleaned Dataset:\\n\", ab_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208699d",
   "metadata": {},
   "source": [
    "chatbot link:https://chatgpt.com/share/673766f5-a4c4-8007-9917-21b68361a4f4\n",
    "chatbot summery:The Amazon Books dataset was pre-processed to ensure consistency and readiness for analysis. Irrelevant columns Weight_oz, Width, and Height were removed to focus on meaningful features. All rows containing missing values (NaN) were dropped to maintain data integrity and ensure accurate computations. Additionally, data types were refined: Pub yearand NumPages were converted to integers (int) for numerical operations, and Hard_or_Paper was redefined as a categorical variable (categore) to better represent its qualitative nature. These steps improved the dataset's quality, making it suitable for further exploratory analysis and modeling while preserving critical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.1\n",
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Simulated dataset (replace with your actual dataset)\n",
    "data = {\n",
    "    \"feature1\": range(1, 101),\n",
    "    \"feature2\": range(101, 201),\n",
    "    \"label\": [0, 1] * 50\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating an 80/20 split with a random seed for reproducibility\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reporting the sizes of the datasets\n",
    "print(f\"Number of observations in training set: {train.shape[0]}\")\n",
    "print(f\"Number of observations in testing set: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87b42c",
   "metadata": {},
   "source": [
    "4.2 output the following:\n",
    "Training dataset: Number of rows in X_train.\n",
    "Testing dataset: Number of rows in X_test.\n",
    "Since the split is 80/20, the total number of observations in the dataset will be divided accordingly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a17f51",
   "metadata": {},
   "source": [
    "4.3 the two steps given for fitting the DecisionTreeClassifier model:\n",
    "1)y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']:This converts the categorical variable Hard_or_Paper into a binary variable.\n",
    "2)'H' corresponds to \"Hardcover\" books. The new y will have:1 for \"Hardcover.\"0 for \"Paperback.\"\n",
    "X = ab_reduced_noNaN[['List Price']]:\n",
    "3)This selects the column List Price as the only feature (independent variable) for training the decision tree model.These steps prepare the data for training by setting y as the target variable and X as the input feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d92c46",
   "metadata": {},
   "source": [
    "4.4 The reason use the training dataset (ab_reduced_noNaN_train) and not the full dataset (ab_reduced_noNaN) to fit the model is 1)Avoid Overfitting: Training the model on the entire dataset would mean there is no independent data left to evaluate the model. This can lead to overfitting, where the model performs well on seen data but poorly on unseen data.2)Test Generalization: By splitting the dataset into training and testing sets, we can test how well the model generalizes to new data it has not seen during training.3)Reproducibility: The training dataset represents the data used to fit the model, while the testing dataset acts as a holdout to simulate real-world scenarios. This ensures that the model's performance metrics are realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ba90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.5\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(\n",
    "    clf, \n",
    "    feature_names=['List Price'],  # Feature name\n",
    "    class_names=['Paperback', 'Hardcover'],  # Class names\n",
    "    filled=True,  # Fill colors to represent classes\n",
    "    rounded=True,  # Rounded corners for clarity\n",
    "    precision=2  # Precision for floating-point numbers\n",
    ")\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0479823",
   "metadata": {},
   "source": [
    "4.6\n",
    "Explanation of Predictions:\n",
    "Each node in the decision tree represents a split based on List Price.\n",
    "At each split:The condition (e.g., List Price <= 25.5) is evaluated.\n",
    "Samples satisfying the condition go to the left child node, while others go to the right.\n",
    "The tree continues splitting until reaching a leaf node and leaf nodes represent the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6153833",
   "metadata": {},
   "source": [
    "chatbot link:https://chatgpt.com/share/6737688c-b530-8007-97fe-9e03e7bf7766\n",
    "chatbot summery:This session focused on creating an 80/20 train-test split for a dataset using Python's train_test_split function from sklearn, ensuring reproducibility with a random seed (random_state=42). A simulated dataset demonstrated the process, which yielded 80 observations in the training set and 20 in the testing set. The session also clarified the concept of an \"observation\" as a row of data representing all features for a single entity. The key takeaway was how to split datasets effectively while maintaining consistent results. Example code was provided for practical implementation, adaptable for the user’s actual dataset (ab_reduced_noNaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "5\n",
    "5.1\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mocking the structure of ab_reduced_noNaN for demonstration purposes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample DataFrame resembling the specifications\n",
    "np.random.seed(42)\n",
    "ab_reduced_noNaN = pd.DataFrame({\n",
    "    'NumPages': np.random.randint(100, 500, size=100),\n",
    "    'Thick': np.random.randint(1, 5, size=100),\n",
    "    'List Price': np.random.uniform(10, 100, size=100),\n",
    "    'Category': np.random.choice(['A', 'B', 'C'], size=100)  # Target variable\n",
    "})\n",
    "\n",
    "# Features and target\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "y = ab_reduced_noNaN['Category']\n",
    "\n",
    "# Creating and fitting the DecisionTreeClassifier\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X, y)\n",
    "\n",
    "# Visualizing the Decision Tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(clf2, feature_names=X.columns, class_names=clf2.classes_, filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d720455",
   "metadata": {},
   "source": [
    "5.2\n",
    "Root Node Decision:\n",
    "The tree starts at the root node, where it evaluates a specific feature against a threshold.\n",
    "Depending on whether the condition is true or false, the sample proceeds to the left or right child node.\n",
    "Intermediate Nodes:\n",
    "Each subsequent node further splits the data based on another feature and threshold, refining the classification based on the training data patterns.\n",
    "This process continues until reaching a terminal nodeor the maximum depth of 4is reached.\n",
    "Leaf Nodes:\n",
    "At each leaf node, the model assigns the class label that dominates the samples reaching that node.\n",
    "The proportion of classes at each leaf provides the confidence of the prediction.\n",
    "Feature Importance:\n",
    "Features higher up in the tree (closer to the root) generally have a stronger influence on predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.1\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate predictions for clf and clf2 using the test data\n",
    "y_test = ab_reduced_noNaN_test['life_exp_good']  # Replace with the actual test labels\n",
    "y_pred_clf = clf.predict(ab_reduced_noNaN_test[['List Price']])\n",
    "y_pred_clf2 = clf2.predict(ab_reduced_noNaN_test[['NumPages', 'Thick', 'List Price']])\n",
    "\n",
    "# Calculate confusion matrices\n",
    "cm_clf = confusion_matrix(y_test, y_pred_clf, labels=[0, 1])\n",
    "cm_clf2 = confusion_matrix(y_test, y_pred_clf2, labels=[0, 1])\n",
    "\n",
    "# Visualize confusion matrices\n",
    "ConfusionMatrixDisplay(cm_clf, display_labels=['Paper', 'Hard']).plot()\n",
    "ConfusionMatrixDisplay(cm_clf2, display_labels=['Paper', 'Hard']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ca69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.2\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to calculate evaluation metrics\n",
    "def calculate_metrics(cm):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    sensitivity = TP / (TP + FN)  # True Positive Rate\n",
    "    specificity = TN / (TN + FP)  # True Negative Rate\n",
    "    accuracy = (TP + TN) / cm.sum()  # Overall accuracy\n",
    "    return round(sensitivity, 3), round(specificity, 3), round(accuracy, 3)\n",
    "\n",
    "# Metrics for clf\n",
    "sensitivity_clf, specificity_clf, accuracy_clf = calculate_metrics(cm_clf)\n",
    "print(f\"clf -> Sensitivity: {sensitivity_clf}, Specificity: {specificity_clf}, Accuracy: {accuracy_clf}\")\n",
    "\n",
    "# Metrics for clf2\n",
    "sensitivity_clf2, specificity_clf2, accuracy_clf2 = calculate_metrics(cm_clf2)\n",
    "print(f\"clf2 -> Sensitivity: {sensitivity_clf2}, Specificity: {specificity_clf2}, Accuracy: {accuracy_clf2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f530529",
   "metadata": {},
   "source": [
    "6.3\n",
    "1)As the result, for clf, Sensitivity is 0.769, Specificity is 0.714 and Accuracy is 0.750. For clf2, Sensitivity is 0.875, Specificity is 0.875 and Accuracy is 0.875.\n",
    "2)Interpretation \n",
    "clf:Sensitivity and specificity are lower, indicating the model struggles to identify both classes accurately.\n",
    "Accuracy is relatively low due to fewer features.\n",
    "clf2:Higher sensitivity, specificity, and accuracy due to the use of additional features (NumPages, Thick, and List Price), allowing better class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1a5eb",
   "metadata": {},
   "source": [
    "chatbot link:https://chatgpt.com/c/673bb726-f6bc-8007-baf2-e3ef44ca5b1c\n",
    "chatbot summery: We analyzed two classifiers, `clf` (baseline) and `clf2` (decision tree), to predict whether a book is hardcover or paperback. Performance metrics were computed from their confusion matric\n",
    "The decision tree (`clf2`) was trained with features `NumPages`, `Thick`, and `List Price` using a max depth of 4. Its structure was visualized using `plot_tree`.We clarified:Positive: hardcover (1); Negative: paperback (0).   Confusion matrix structure:   \\[\\begin{bmatrix} TN & FP \\\\ FN & TP \\end{bmatrix} \\]  - `confusion_matrix` requires `y_true` first, then `y_pred`.This comparison aids model evaluation and highlights areas for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd5988",
   "metadata": {},
   "source": [
    "7.1 1)The differences between the two confusion matrices arise from the models' use of different sets of features to predict the target variable life_exp_good. The first confusion matrix uses only the 'List Price' feature, which is likely insufficient to fully capture the underlying factors affecting the classification of life_exp_good. So, the model has limited predictive power, leading to higher false positives and false negatives, and overall poorer classification performance.2)The second confusion matrix, uses a combination of features—'NumPages', 'Thick', and 'List Price'. By incorporating these additional features, the model gains access to more relevant information about the relationships between the features and the target. This expanded feature set allows the model to better distinguish between the classes, leading to improved true positive and true negative rates and fewer classification errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956fbaf",
   "metadata": {},
   "source": [
    "7.2 The confusion matrix for clf2 is better because it uses a richer set of features, leading to improved decision boundaries and a more generalizable model. By incorporating more relevant data, the model can make more informed splits, thereby reducing error rates and improving overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d38ad",
   "metadata": {},
   "source": [
    "chatbot link:https://chatgpt.com/c/673bc476-8bc8-8007-98bf-aa5dff97e1d0\n",
    "chatbot summery:The performance differences between the two models and their confusion matrices stem from the number and relevance of features used for predictions. \n",
    "Model with 'List Price' Only: This model relies solely on a single feature, 'List Price', which provides limited information about the target (life_exp_good). Consequently, it suffers from higher misclassification rates, as shown in the confusion matrix.\n",
    "Model with 'List Price', 'NumPages', and 'Thick': Incorporating additional features ('NumPages' and 'Thick') improves the model's ability to capture more complex relationships in the data. This results in a significant reduction in classification errors and an overall improvement in performance, reflected in the better confusion matrix.\n",
    "The improved performance of the second model highlights the importance of selecting relevant and diverse features to enhance classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45e2fe",
   "metadata": {},
   "source": [
    "8 \n",
    "8.1 To visualize feature importances,follow these steps:\n",
    "Use clf2.feature_importances_ to retrieve the importance values of features.\n",
    "Use clf2.feature_names_in_ to match the importance values to their corresponding feature names.\n",
    "Sort and Rank Features: Sort features by their importance scores for better visualization and understanding.\n",
    "Visualize with a Bar Chart: Create a bar chart where the x-axis lists the feature names and the y-axis represents their importance scores.\n",
    "8.2 The most important variable is the one with the highest value in clf2.feature_importances_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d5bdb",
   "metadata": {},
   "source": [
    "chatbot link: https://chatgpt.com/c/673bc7e7-a074-8007-9f6c-9ec0c661c976"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244d1b6",
   "metadata": {},
   "source": [
    "9\n",
    "In linear regression, coefficients represent the change in the predicted outcome associated with a one-unit change in a predictor variable, assuming all other variables are held constant, which allows for straightforward interpretation of the direct influence of each variable on the outcome. In contrast, feature importance in decision trees is derived from how often and effectively a variable is used to split the data, which measures its relative contribution to reducing impurity in the model but does not directly quantify the effect of changes in that variable on the predicted outcome. This makes linear regression coefficients more interpretable in terms of causal or direct relationships, whereas feature importance reflects a variable's overall predictive usefulness in the context of the tree's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c617b",
   "metadata": {},
   "source": [
    "chatbot link: https://chatgpt.com/c/673bd5a1-85b0-8007-8ada-3cd67524b052"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8a796",
   "metadata": {},
   "source": [
    "10 Yes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
